{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f70fd2a9-d439-47ff-ab71-d89fb40eb901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "325b2d60-9f59-4803-8670-627a668e1fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "friends = pd.read_csv('friends_transcripts.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "89fa9493-e92e-484f-b19b-46c164b261c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season_id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>scene_id</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>tokens</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s01</td>\n",
       "      <td>e01</td>\n",
       "      <td>c01</td>\n",
       "      <td>u001</td>\n",
       "      <td>Monica Geller</td>\n",
       "      <td>[['There', \"'s\", 'nothing', 'to', 'tell', '!']...</td>\n",
       "      <td>There's nothing to tell! He's just some guy I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s01</td>\n",
       "      <td>e01</td>\n",
       "      <td>c01</td>\n",
       "      <td>u002</td>\n",
       "      <td>Joey Tribbiani</td>\n",
       "      <td>[[\"C'mon\", ',', 'you', \"'re\", 'going', 'out', ...</td>\n",
       "      <td>C'mon, you're going out with the guy! There's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s01</td>\n",
       "      <td>e01</td>\n",
       "      <td>c01</td>\n",
       "      <td>u003</td>\n",
       "      <td>Chandler Bing</td>\n",
       "      <td>[['All', 'right', 'Joey', ',', 'be', 'nice', '...</td>\n",
       "      <td>All right Joey, be nice. So does he have a hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s01</td>\n",
       "      <td>e01</td>\n",
       "      <td>c01</td>\n",
       "      <td>u004</td>\n",
       "      <td>Phoebe Buffay</td>\n",
       "      <td>[['Wait', ',', 'does', 'he', 'eat', 'chalk', '...</td>\n",
       "      <td>Wait, does he eat chalk?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s01</td>\n",
       "      <td>e01</td>\n",
       "      <td>c01</td>\n",
       "      <td>u005</td>\n",
       "      <td>unknown</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  season_id episode_id scene_id utterance_id         speaker  \\\n",
       "0       s01        e01      c01         u001   Monica Geller   \n",
       "1       s01        e01      c01         u002  Joey Tribbiani   \n",
       "2       s01        e01      c01         u003   Chandler Bing   \n",
       "3       s01        e01      c01         u004   Phoebe Buffay   \n",
       "4       s01        e01      c01         u005         unknown   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [['There', \"'s\", 'nothing', 'to', 'tell', '!']...   \n",
       "1  [[\"C'mon\", ',', 'you', \"'re\", 'going', 'out', ...   \n",
       "2  [['All', 'right', 'Joey', ',', 'be', 'nice', '...   \n",
       "3  [['Wait', ',', 'does', 'he', 'eat', 'chalk', '...   \n",
       "4                                                 []   \n",
       "\n",
       "                                          transcript  \n",
       "0  There's nothing to tell! He's just some guy I ...  \n",
       "1  C'mon, you're going out with the guy! There's ...  \n",
       "2  All right Joey, be nice. So does he have a hum...  \n",
       "3                           Wait, does he eat chalk?  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 50) #50 characters in the data frame\n",
    "data=pd.DataFrame.from_dict(friends)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb7f35ef-0a66-457c-bb1f-ba809c3d00f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/6c06fdb6-5d00-4caf-9b1f-\n",
      "[nltk_data]     b5d1ef16fd97/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/6c06fdb6-5d00-4caf-9b1f-\n",
      "[nltk_data]     b5d1ef16fd97/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "83b6feda-ac8a-4621-9639-b16bdeb2e6f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(text)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# using function to clean text data\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscript\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscript\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: data_cleaning(x))\n\u001b[1;32m     38\u001b[0m data\u001b[38;5;241m.\u001b[39mhead() \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Eliminate any row with no entry fordata['Lines'] after cleaning \u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[136], line 37\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(text)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# using function to clean text data\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscript\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscript\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: data_cleaning(x))\n\u001b[1;32m     38\u001b[0m data\u001b[38;5;241m.\u001b[39mhead() \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Eliminate any row with no entry fordata['Lines'] after cleaning \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[83], line 4\u001b[0m, in \u001b[0;36mdata_cleaning\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_cleaning\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# lower text\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# tokenize text for space and \\n\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#text = re.split('\\s+|\\n',text)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# remove words that contain numbers\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     text \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(c\u001b[38;5;241m.\u001b[39misdigit() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m word)]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "\n",
    "    # tokenize text for space and \\n\n",
    "    text = re.split('\\s+|\\n',text)\n",
    "    \n",
    "    # remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text]\n",
    "\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "\n",
    "    # remove stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    text = [x for x in text if x not in stop]\n",
    "\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "\n",
    "    # pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "\n",
    "    # create a (word, pos_tag) tuple for each word in text and then lemmatize text\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    return(text)\n",
    "\n",
    "# using function to clean text data\n",
    "data['transcript'] = data['transcript'].apply(lambda x: data_cleaning(x))\n",
    "data.head() \n",
    "\n",
    "# Eliminate any row with no entry fordata['Lines'] after cleaning \n",
    "data['transcript'].replace('', np.NaN, inplace=True)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "abdbc7ee-3162-48de-9cfb-d90d5346ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a915e290-d6ac-49ae-9a20-56a0c0774683",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m count_vector \u001b[38;5;241m=\u001b[39m CountVectorizer(analyzer\u001b[38;5;241m=\u001b[39mword_tokenize)\n\u001b[0;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m count_vector\u001b[38;5;241m.\u001b[39mfit_transform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscript\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m             )\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1269\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:105\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     doc \u001b[38;5;241m=\u001b[39m decoder(doc)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:238\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    235\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "#implementing count vectorisation\n",
    "count_vector = CountVectorizer(analyzer=word_tokenize)\n",
    "X = count_vector.fit_transform(data['transcript'])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196fedd-ffbb-40a1-8684-d32a983b15a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing tf-idf word embedding\n",
    "def tfidf(data):\n",
    "    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', ngram_range=(1, 2))\n",
    "\n",
    "    tfidf_result = tfidf.fit_transform(data).toarray()\n",
    "    tfidf_features = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\n",
    "    tfidf_features.index = data.index\n",
    "    return tfidf_features\n",
    "\n",
    "\n",
    "tfidf_df = tfidf(data_thres['Lines'])\n",
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c3d7f-a732-483c-b03a-511b68ab4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating word embedding techniques\n",
    "Eval_score_embedding = {'Count Vectorizer': [], 'TF-IDF':[] }\n",
    "name_embedded_dataframes = {'Count Vectorizer': count_vectorizer_df,'TF-IDF': tfidf_df}\n",
    "\n",
    "def word_embedding_performance_eval(clf,dataframe):\n",
    "    Eval_score =  cross_val_score(clf,dataframe, target_thres, cv=5)\n",
    "    return  Eval_score.mean() \n",
    "    \n",
    "model =  LogisticRegression(max_iter = 1e6)\n",
    "for dataframe in name_embedded_dataframes.keys():\n",
    "    Eval_score_embedding [dataframe] = word_embedding_performance_eval(model,name_embedded_dataframes[dataframe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadab37a-0a55-494e-b611-e6722e19f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "leb = LabelEncoder()\n",
    "y = leb.fit_transform(data['speaker'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba9900-bb1a-48fa-8205-9d462827c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_clf = MultinomialNB(force_alpha=True)\n",
    "mnb_clf = mnb.clf.fit(X_train, y_train)\n",
    "\n",
    "#Testing Classifier\n",
    "\n",
    "y-pred= mnb_clf.picredt(X_test)\n",
    "acc = np.sum((y_pred == y_test).astype(int))/y_pred.size\n",
    "print(\"accuracy: %4f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47cf39-5517-4022-9ee1-3e2f20723cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding precision, Recall and F1-score\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "\n",
    "p_micro, r_micro, f1_micro, _ = prfs(y_pred=pred_NB, y_true=y_test, average=\"micro\")\n",
    "print(\"Micro Evaluation: Precision: %.4f; Recall: %.4f; F1-Score: %.4f\" % (p_micro, r_micro, f1_micro))\n",
    "\n",
    "p_macro, r_macro, f1_macro, _ = prfs(y_pred=pred_NB, y_true=y_test, average=\"macro\")\n",
    "print(\"Macro Evaluation: Precision: %.4f; Recall: %.4f; F1-Score: %.4f\" % (p_macro, r_macro, f1_macro))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
